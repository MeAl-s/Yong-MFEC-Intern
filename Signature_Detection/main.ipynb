{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.png to C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# Function to check and convert image to JPEG\n",
    "def check_and_convert_image(image_path, output_path):\n",
    "    # Check if the image is already a JPEG\n",
    "    if not image_path.lower().endswith('.jpg') and not image_path.lower().endswith('.jpeg'):\n",
    "        # Read the original image\n",
    "        image = cv2.imread(image_path)\n",
    "        # Convert and save the image as a high-quality JPEG\n",
    "        cv2.imwrite(output_path, image, [int(cv2.IMWRITE_JPEG_QUALITY), 95])\n",
    "        print(f\"Converted {image_path} to {output_path}\")\n",
    "        return output_path\n",
    "    else:\n",
    "        print(f\"Image {image_path} is already a JPEG\")\n",
    "        return image_path\n",
    "\n",
    "# Paths\n",
    "image_path = r\"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.png\"\n",
    "jpeg_image_path = r\"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.jpg\"\n",
    "annotated_image_path = r'C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.jpg'\n",
    "phrases_path = r\"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\identified_phrases.jsonl\"\n",
    "\n",
    "\n",
    "# Check and convert image if necessary\n",
    "final_image_path = check_and_convert_image(image_path, jpeg_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(710, 1096, 3)\n",
      "Enhanced image saved at C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.jpg\n"
     ]
    }
   ],
   "source": [
    "# Enhance image quality for better OCR results\n",
    "def enhance_image(image_path, output_path):\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    print(image.shape)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Unable to read image at {enhanced_image_path}\")\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Apply GaussianBlur to reduce noise and improve text detection\n",
    "    blurred_image = cv2.GaussianBlur(gray_image, (5, 5), 0)\n",
    "    \n",
    "    # Apply adaptive thresholding to enhance the text regions\n",
    "    thresh_image = cv2.adaptiveThreshold(blurred_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n",
    "    \n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(output_path, thresh_image)\n",
    "    print(f\"Enhanced image saved at {output_path}\")\n",
    "\n",
    "# Paths\n",
    "enhanced_image_path = r\"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.jpg\"\n",
    "\n",
    "# Enhance the image\n",
    "enhance_image(final_image_path, enhanced_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ชนิดรถ.......\n",
      "เลขเครื่องยนต์...\n",
      "ขอแจ้งต่อนายทะเบียนว่า ผู้โอนได้โอนรถคันเลขทะเบียน......\n",
      "เลขตัวรถ.\n",
      "จังหวัด.\n",
      "..ชนิตเครื่องยนต์...\n",
      "...ให้แก่ผู้รับโอนแล้ว\n",
      "โดยชื่อขายในราคา..............\n",
      "...บาท (.....\n",
      "พร้อมนี้ได้แนบหลักฐานประกอบคำขอ ดังนี้\n",
      "1.\n",
      ". 2.\n",
      "3.\n",
      "5.\n",
      "ลงชื่อ\n",
      "อ.\n",
      "4.\n",
      "6.\n",
      "..ผู้โอน\n",
      "ลง อ.\n",
      "ผู้รับโอน\n",
      "...ผู้รับโอน\n",
      "ลงซอ..\n",
      "..ผู้ยื่นคําขอ\n",
      "ขอรับรองว่าเป็นลายมือชื่อของผู้โอนจริง\n",
      "ลงซอ.....\n",
      "ชนิด\n",
      "รถ\n",
      ".......\n",
      "เลข\n",
      "เครื่องยนต์\n",
      "...\n",
      "ขอ\n",
      "แจ้ง\n",
      "ต่อ\n",
      "นาย\n",
      "ทะเบียน\n",
      "ว่า\n",
      "ผู้\n",
      "โอน\n",
      "ได้\n",
      "โอน\n",
      "รถ\n",
      "คัน\n",
      "เลข\n",
      "ทะเบียน\n",
      "......\n",
      "เลข\n",
      "ตัว\n",
      "รถ\n",
      ".\n",
      "จังหวัด\n",
      ".\n",
      "..\n",
      "ช\n",
      "นิต\n",
      "เครื่องยนต์\n",
      "...\n",
      "...\n",
      "ให้\n",
      "แก่\n",
      "ผู้รับ\n",
      "โอน\n",
      "แล้ว\n",
      "โดย\n",
      "ชื่อ\n",
      "ขาย\n",
      "ใน\n",
      "ราคา\n",
      "..............\n",
      "...\n",
      "บาท\n",
      "(\n",
      ".....\n",
      "พร้อม\n",
      "นี้\n",
      "ได้\n",
      "แนบ\n",
      "หลักฐาน\n",
      "ประกอบ\n",
      "คำขอ\n",
      "ดังนี้\n",
      "1\n",
      ".\n",
      ".\n",
      "2\n",
      ".\n",
      "3\n",
      ".\n",
      "5\n",
      ".\n",
      "ลงชื่อ\n",
      "อ\n",
      ".\n",
      "4\n",
      ".\n",
      "6\n",
      ".\n",
      "..\n",
      "ผู้\n",
      "โอน\n",
      "ลง\n",
      "อ\n",
      ".\n",
      "ผู้รับ\n",
      "โอน\n",
      "...\n",
      "ผู้รับ\n",
      "โอน\n",
      "ลง\n",
      "ซอ\n",
      "..\n",
      "..\n",
      "ผู้\n",
      "ยื่น\n",
      "คํา\n",
      "ขอ\n",
      "ขอ\n",
      "รับรอง\n",
      "ว่า\n",
      "เป็น\n",
      "ลายมือชื่อ\n",
      "ของ\n",
      "ผู้\n",
      "โอน\n",
      "จริง\n",
      "ลง\n",
      "ซอ\n",
      ".....\n",
      "Text has been written to extracted_texts.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "import google.cloud\n",
    "import cv2\n",
    "\n",
    "# Set up the environment variable for Google Cloud authentication\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\signature-429903-456702e081c7.json\"\n",
    "\n",
    "# Initialize the Google Cloud Vision client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "def detect_text(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(f'{response.error.message}')\n",
    "\n",
    "    return texts\n",
    "\n",
    "# Example usage\n",
    "texts = detect_text(enhanced_image_path)\n",
    "for text in texts:\n",
    "    print(text.description)\n",
    "    # Open a file in write mode\n",
    "with open('extracted_texts.txt', 'w', encoding='utf-8') as file:\n",
    "    # Iterate through each text annotation and write the description to the file\n",
    "    for text in texts:\n",
    "        file.write(text.description + \"\\n\")\n",
    "\n",
    "print(\"Text has been written to extracted_texts.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Azure OpenAI Configuration\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://insideout.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"b6c0397877a9420aa7b3c2bad2e622f7\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"]\n",
    ")\n",
    "\n",
    "extracted_text = texts[0].description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: ลงชื่อ\n",
      "ผู้โอน\n",
      "ผู้รับโอน\n",
      "ผู้ยื่นคำขอ\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "            model=\"gpt4o\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"\"\"You are an assistant that helps analyze document text to identify key phrases indicating where signatures are needed, make sure to cover every single possible signature, especially if there're full stops or dot dot lines within the text you analyzed for example the text \"...ลงชื่อ\" has a ... in it which means that its a place where people can sign. Once you have identified the key phrases make sure they are not only one single characters, they must be a word consisted in the dictionary, they cannot be words that do not make sense, you should only list the key phrases without using numbers, bullet points, or any other form of listing.\"\"\"\n",
    "                },\n",
    "                {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": extracted_text\n",
    "        }\n",
    "            ]\n",
    "        )\n",
    "\n",
    "print(\"OpenAI Response:\", response.choices[0].message.content.strip())\n",
    "\n",
    "identified_phrases = response.choices[0].message.content.strip()\n",
    "# Save identified phrases as JSONL\n",
    "with open(\"identified_phrases.jsonl\", \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump({\"phrases\": identified_phrases}, file, ensure_ascii=False)\n",
    "    file.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[re.compile('ลงชื่อ'), re.compile('ผู้โอน'), re.compile('ผู้รับโอน'), re.compile('ผู้ยื่นคำขอ')]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "with open(phrases_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "    phrases = data[\"phrases\"].split('\\n')\n",
    "\n",
    "# Clean up phrases to ensure no leading/trailing whitespaces\n",
    "words_to_annotate = ['ลงชื่อ', 'ผู้โอน', 'ผู้รับโอน', 'ผู้ยื่นคำขอ']\n",
    "\n",
    "# Create patterns for the exact words\n",
    "patterns = [re.compile(re.escape(word)) for word in words_to_annotate]\n",
    "\n",
    "\n",
    "# Compile regex patterns for the phrases\n",
    "print(patterns)\n",
    "\n",
    "image = cv2.imread(enhanced_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated image saved to C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\data\\\\s8oy9vxt3fVu21x5dbo0-o.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "import cv2\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "# Set up the environment variable for Google Cloud authentication\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\signature-429903-456702e081c7.json\"\n",
    "\n",
    "# Initialize the Google Cloud Vision client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "def detect_words_with_boxes(texts, patterns):\n",
    "    detected_boxes = []\n",
    "    for text in texts[1:]:  # Skip the first element as it contains the whole text\n",
    "        for pattern in patterns:\n",
    "            if pattern.fullmatch(text.description.strip()):\n",
    "                vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "                detected_boxes.append((vertices, text.description))\n",
    "    return detected_boxes\n",
    "\n",
    "def annotate_image(image_path, detected_boxes, annotated_image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    for (vertices, word) in detected_boxes:\n",
    "        pts = np.array(vertices, np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        cv2.polylines(image, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "    cv2.imwrite(annotated_image_path, image)\n",
    "\n",
    "# Detect words and their bounding boxes\n",
    "detected_boxes = detect_words_with_boxes(texts, patterns)\n",
    "\n",
    "# Annotate and save the image\n",
    "annotate_image(enhanced_image_path, detected_boxes, annotated_image_path)\n",
    "\n",
    "print(f\"Annotated image saved to {annotated_image_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/data/image.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 66\u001b[0m\n\u001b[0;32m     63\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(annotated_image_path, image)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Path to the enhanced image\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43menhanced_image_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Print detected texts for debugging\u001b[39;00m\n\u001b[0;32m     69\u001b[0m print_detected_texts(texts)\n",
      "Cell \u001b[1;32mIn[35], line 15\u001b[0m, in \u001b[0;36mdetect_text\u001b[1;34m(image_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetect_text\u001b[39m(image_path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[vision\u001b[38;5;241m.\u001b[39mEntityAnnotation]:\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m image_file:\n\u001b[0;32m     16\u001b[0m         content \u001b[38;5;241m=\u001b[39m image_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     18\u001b[0m     image \u001b[38;5;241m=\u001b[39m vision\u001b[38;5;241m.\u001b[39mImage(content\u001b[38;5;241m=\u001b[39mcontent)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\Downloads\\Yong-MFEC-Intern\\myenv\\lib\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/data/image.png'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.cloud import vision\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "# Set up the environment variable for Google Cloud authentication\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\signature-429903-456702e081c7.json\"\n",
    "\n",
    "# Initialize the Google Cloud Vision client\n",
    "client = vision.ImageAnnotatorClient()\n",
    "\n",
    "def detect_text(image_path: str) -> List[vision.EntityAnnotation]:\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = vision.Image(content=content)\n",
    "    response = client.text_detection(image=image)\n",
    "    texts = response.text_annotations\n",
    "\n",
    "    if response.error.message:\n",
    "        raise Exception(f'API Error: {response.error.message}')\n",
    "\n",
    "    return texts\n",
    "\n",
    "def print_detected_texts(texts: List[vision.EntityAnnotation]):\n",
    "    print(\"Detected Texts:\")\n",
    "    for text in texts:\n",
    "        print(f\"Description: {text.description}\")\n",
    "        print(f\"Bounding Poly: {[(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "def describe_location(texts: List[vision.EntityAnnotation], keywords: List[str]) -> str:\n",
    "    locations = []\n",
    "    for text in texts:\n",
    "        if text.description in keywords:\n",
    "            vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "            if vertices:\n",
    "                x_avg = sum([v[0] for v in vertices]) / len(vertices)\n",
    "                y_avg = sum([v[1] for v in vertices]) / len(vertices)\n",
    "                locations.append((text.description, x_avg, y_avg))\n",
    "    description = \"Keywords found at approximate locations:\\n\"\n",
    "    for word, x, y in locations:\n",
    "        description += f\"{word}: located around ({x}, {y})\\n\"\n",
    "    return description\n",
    "\n",
    "def detect_words_with_boxes(texts: List[vision.EntityAnnotation], patterns: List[re.Pattern]) -> List[Tuple[List[Tuple[int, int]], str]]:\n",
    "    detected_boxes = []\n",
    "    for text in texts[1:]:  # Skip the first element as it contains the whole text\n",
    "        for pattern in patterns:\n",
    "            if pattern.fullmatch(text.description.strip()):\n",
    "                vertices = [(vertex.x, vertex.y) for vertex in text.bounding_poly.vertices]\n",
    "                detected_boxes.append((vertices, text.description))\n",
    "    return detected_boxes\n",
    "\n",
    "def annotate_image(image_path: str, detected_boxes: List[Tuple[List[Tuple[int, int]], str]], annotated_image_path: str):\n",
    "    image = cv2.imread(image_path)\n",
    "    for (vertices, word) in detected_boxes:\n",
    "        pts = np.array(vertices, np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))\n",
    "        cv2.polylines(image, [pts], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "    cv2.imwrite(annotated_image_path, image)\n",
    "\n",
    "# Path to the enhanced image\n",
    "texts = detect_text(enhanced_image_path)\n",
    "\n",
    "# Print detected texts for debugging\n",
    "print_detected_texts(texts)\n",
    "\n",
    "# Define the words we want to annotate\n",
    "words_to_annotate = ['ลงชื่อ', 'ผู้โอน', 'ผู้รับโอน', 'ผู้ยื่นคำขอ']\n",
    "\n",
    "# Create patterns for the exact words\n",
    "patterns = [re.compile(re.escape(word)) for word in words_to_annotate]\n",
    "\n",
    "# Generate a description of the approximate locations\n",
    "description = describe_location(texts, words_to_annotate)\n",
    "print(description)\n",
    "\n",
    "# Save the description to a text file\n",
    "with open('C:\\\\Users\\\\ACER\\\\Downloads\\\\Yong-MFEC-Intern\\\\Signature_Detection\\\\extracted_texts.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(description)\n",
    "\n",
    "# Detect words and their bounding boxes\n",
    "detected_boxes = detect_words_with_boxes(texts, patterns)\n",
    "\n",
    "annotate_image(enhanced_image_path, detected_boxes, annotated_image_path)\n",
    "\n",
    "print(f\"Annotated image saved to {annotated_image_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
